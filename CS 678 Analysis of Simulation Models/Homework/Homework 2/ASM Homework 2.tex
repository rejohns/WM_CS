\documentclass[11pt]{article} % 
\usepackage[pdftex]{graphicx}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{graphics}
\usepackage{psfrag}
\usepackage{pgf}
\usepackage{color}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usepackage[latin1]{inputenc}
\usepackage{amsthm}
\usepackage{amsmath,amssymb}
\usepackage{enumerate}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{9in}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\tab}{\;\;\;\;\;}
\newcommand{\inv}{^{-1}}
\newcommand{\tr}{\textrm}
\newcommand{\lc}{\sqcup}
\newcommand{\var}{\tr{Var}}
\newcommand{\cov}{\tr{Cov}}

\begin{document}

\hfill Robert Johns

\hfill January 30, 2014

\begin{center} {\Large CSCI 678: Statistical Analysis of Simulation Models}\\{\large Homework 2}\end{center}

\begin{enumerate}

%1
\item Let $X$ and $Y$ be random variables with finite means and variances.  Six results concerning expected values are given below.  Pick three of the results and show that they are true for \emph{continuous} random variables.  Assume that $k$ is constant.

\begin{enumerate}

%1a
\item $E[kX] = k * E[X]$

{\bf Solution:} We have $E[kX] = \int_{-\infty}^\infty kxf(x)\;dx$.  Since $k$ is a constant, we can pull it out of the integral to get $E[kX] = k\int_{-\infty}^\infty xf(x)\;dx = kE[X]$

%1b
\item $E[k] = k$

{\bf Solution:} We have $E[k] = \int_{-\infty}^\infty kf(x)\;dx$.  Since $k$ is a constant, we can pull it out of the integral to get $E[k] = k\int_{-\infty}^\infty f(x)\;dx = k$ because $f$ is a probability density function and integrates to 1 over the reals.

%1c
\setcounter{enumii}{3}
\item $V[X] = E[X^2] - \mu^2$

{\bf Solution:} We have $V[X] = E[(X - \mu)^2] = E[X^2 - 2\mu X + \mu^2] = E[X^2] - 2\mu E[X] + \mu^2 = E[X^2] - 2\mu^2 + \mu^2 = E[X^2] - \mu^2$

\end{enumerate}

%2
\item Show that the following relationships are true using pages 2.9-2.11 in the class notes:

\begin{enumerate}

%2a
\item The Rayleigh is a special case of the Weibull distribution.

{\bf Solution:} We have the pdf for the Weibull distribution: $f(x) = (\beta/\alpha)x^{\beta - 1} \exp[-(1/\alpha)x^\beta]$.  If we set $\beta = 2$, we now have $f(x) = (2x/\alpha)\exp[-(x^2/\alpha)]$, which is the pdf for the Rayleigh distribution.

%2b
\item The square root of an exponential random variable has a Rayleigh distribution.

{\bf Solution:} Let the random variable $X$ be distributed exponentially.  We know that the pdf of $X$ is $f_X(x) = (1/\alpha)\exp[-(x^2/\alpha)]$.  If we apply the transformation $Y = g(X) = \sqrt{X}$ (which is bijective across non-negative reals), we see that the inverse $X = g\inv(Y) = Y^2$ has Jacobian $\frac{dX}{dY} = 2Y$.  Applying the transformation technique, we see that the pdf of $Y$ is $f_Y(y) = f_X(g\inv(y))|\frac{dX}{dY}| = (1/\alpha)\exp[-(y^2/\alpha)]|2y| = (2y/\alpha)\exp[-(y^2/\alpha)]$, the pdf of the Rayleigh distribution.

%2c
\item The sum of independent and identically distributed exponential random variables is Erlang.

{\bf Solution:} Let $X_1, X_2, \ldots X_k$ be iid exponential random variables.  Define the random variable $Y$ as $Y = \sum_{i  = 1}^kX_i$.  We have from the mgf technique that $m_Y(t) = \Pi_{i=1}^km_{X_i}(t)$.  We know that the mgf for an exponential random variable is $m_X(t) = (1 - \alpha t)\inv$, so we then have that $m_Y(t) = (1 - \alpha t)^{-k}$, which is the mgf for an Erlang random variable.  

\end{enumerate}

%new page
\newpage

%3
\item For the joint probability density function defined by
$$f(x_1, x_2) = 2 \tab 0 < x_1 < x_2 < 1$$
find the covariance between $X_1$ and $X_2$.

{\bf Solution:} For ease of writing, let's refer to $X_1$ as $X$ and $X_2$ as $Y$.  The shortcut formula for the covariance is $\cov(X,Y) = E[XY] - \mu_X\mu_Y$.  We can find $E[XY]$ by evaluating the following integral: $\int_{0}^{1}\int_{0}^{y}2xy\;dxdy = 1/4$.  To find the expected values of $X$ and $Y$, we need their marginal distributions, which are $f_X(x) = 2 - 2x. f_Y(y) = 2y$, giving us expected values of $E[X] = 1/3, E[Y] = 2/3$, so we have $\cov(X,Y) = \frac{1}{4} - \frac{1}{3}*\frac{2}{3} = \frac{1}{4} - \frac{2}{9} = \frac{1}{36}$

%4
\item Let $X_1, X_2, \ldots, X_n$ be independent random variables and $\overline{X} = \frac{1}{n}\sum_{i = 1}^n X_i$.  Find the exact values of $E[X_i], \var[X_i], E[\overline{X}]$ and $\var[\overline{X}]$ by analytic methods for the three parent populations given in (a), (b) and (c) below.  Also, write a computer program that estimates these four quantities for $n = 5, 50, 500, 5000$ using seven replications of each experiment when:

\begin{enumerate}

%4a
\item $X_1, X_2, \ldots X_n$ are independent $U(0,1)$.

{\bf Solution:} $E[X_i] = \int_0^1x\;dx = .5$

$V[X_i] = E[X_i^2] - E[X_i]^2 = \int_0^1x^2\;dx - 1/4  = 1/12$

$E[(1/n)\sum_{i = 1}^nX_i] = (1/n)\sum_{i = 1}^nE[X_i] = n/(2n) = .5$

$\var[(1/n)\sum_{i = 1}^nX_i] = (1/n^2)\sum_{i =1}^n\var[X_i] = 1/(12n)$

The program \texttt{asm4a.r}, attached at end, is used to calculate the desired values.

%4b
\item $X_1, X_2, \ldots X_n$ are independent variates from a distribution with probability density function $f(x) = 2/x^3$ for $x \ge 1$.

{\bf Solution:} $E[X_i] = \int_1^\infty x(2/x^2)\;dx = 2$

$V[X_i] = E[X_i^2] - E[X_i]^2 = \int_1^\infty x^2(2/x^3)\;dx - 4$ does not converge.

$E[(1/n)\sum_{i = 1}^nX_i] = (1/n)\sum_{i = 1}^nE[X_i] = (2n)/n$ = 2

$\var[\frac{1}{n}\sum_{i = 1}^nX_i] = \frac{1}{n^2}\sum_{i =1}^n\var[X_i]$ does not converge because no variances converge.

The program \texttt{asm4b.r}, attached at end, is used to calculate the desired values.

%4c
\item $X_1, X_2, \ldots X_n$ are independent Cauchy variates.

{\bf Solution:}$E[X_i] = \int_{-\infty}^\infty \frac{x}{\alpha\pi[1 + ((x-\alpha)/\alpha)^2]}\;dx = \alpha$

$V[X_i] = E[X_i^2] - E[X_i]^2 = \int_{-\infty}^\infty \frac{x^2}{\alpha\pi[1 + ((x-\alpha)/\alpha)^2]}\;dx - \alpha^2$ does not converge.

$E[(1/n)\sum_{i = 1}^nX_i] = (1/n)\sum_{i = 1}^nE[X_i] = (n\alpha)/n = \alpha$

$\var[\frac{1}{n}\sum_{i = 1}^nX_i] = \frac{1}{n^2}\sum_{i =1}^n\var[X_i]$ does not converge because no variances converge.

The program \texttt{asm4c.r}, attached at end, is used to calculate the desired values.

\end{enumerate}

%5
\item For any random variables $X_1, X_2$ and any numbers $a_1, a_2$, show that $\var(a_1X_1 + a_2X_2) = a_1^2\var(X_1) + 2a_1a_2\cov(X_1, X_2) + a_2^2\var(X_2)$

{\bf Solution:} For ease of writing, let us refer to $a_1$ as $a$, $a_2$ as $b$, $X_1$ as $X$ and $X_2$ as $Y$.  We have 
$$\var(aX + bY) = E[((aX + bY) - (a\mu_X + b\mu_Y))^2]$$ 
$$= E[(a(X - \mu_X) - b(Y - \mu_Y))^2]$$
$$= E[a^2(X - \mu_X)^2 + 2ab(X - \mu_X)(Y-\mu_Y) + b^2(Y - \mu_Y)^2] $$
$$= E[a^2(X - \mu_X)^2] + 2abE[(X - \mu_X)(Y-\mu_Y)] + b^2 E[(Y - \mu_Y)^2]$$
$$= a^2\var[X] + 2ab\cov[X,Y] + b^2\var[Y]$$

%6
\item Use the R calculator mode to find the following quantities:

\begin{enumerate}

%6a
\item $4 * \arctan(1)$

{\bf Solution:} \begin{verbatim}
> 4 * atan(1.0)
[1] 3.141593
\end{verbatim}

%6b
\item $1 + \lfloor e^3\rfloor$

{\bf Solution:}\begin{verbatim}
> 1 + floor(exp(3))
[1] 21
\end{verbatim}

%6c
\item $\frac{1}{\sqrt{2\pi}}$

{\bf Solution:}\begin{verbatim}
> 1 / sqrt(2 * pi)
[1] 0.3989423
\end{verbatim}

%6d
\item If $Z$ is a standard normal random variable, find the value $a$ such that $P(Z < a) = 0.975$

{\bf Solution:}\begin{verbatim}
> pnorm(1.959964, 0, 1)
[1] 0.975
\end{verbatim}

%6e
\item If $X$ is a random variable having the chi-square distribution with fifteen degrees of freedom, find $P(X < 17.48)$

{\bf Solution:}\begin{verbatim}
> pchisq(17.48, 15)
[1] 0.7090115
\end{verbatim}

%6f
\item Generate 8 random variates from the t distribution with seven degrees of freedom.

{\bf Solution:}\begin{verbatim}
> rchisq(8, 15)
[1] 24.668024 19.978752 15.043670 16.100242 18.429329 16.696524  9.354511
[8] 12.682383
\end{verbatim}

%6g
\item Find the value of the standard normal probability density function at $x = 0$.

{\bf Solution:}\begin{verbatim}
> dnorm(0)
[1] 0.3989423
\end{verbatim}

\end{enumerate}

\end{enumerate}

\end{document}